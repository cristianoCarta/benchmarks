{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from src.benchmarkers import *\n",
    "from src.benchmarkersV2 import *\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from typing import *\n",
    "#import framework_functions as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = pq.read_table(r\"C:\\Users\\Cristiano Lavoro\\Downloads\\ds_10.parquet\")\n",
    "table = pq.read_table(r\"C:\\Users\\Cristiano Lavoro\\Desktop\\benchmarks\\imagenette\\imagenette2\\output\\ds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── image_feature\n",
      "    └── [0]\n",
      "        └── [0]\n",
      "            ├── class_feature\n",
      "            │   ├── [0]\n",
      "            │   │   └── label\n",
      "            ├── image\n",
      "            └── text_feature\n",
      "                └── [0]\n",
      "                    └── text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ff.draw_tree_schema(table.take([0]).to_pydict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = table.schema\n",
    "batches = [recordbatch for recordbatch in table.to_batches(300) if len(recordbatch) > 0]\n",
    "offsets = np.cumsum([0] + [len(b) for b in batches], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 320\n",
    "length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_search(arr: List[int], x: int) -> int:\n",
    "    \"\"\"\n",
    "    Return the position i of a sorted array so that arr[i] <= x < arr[i+1]\n",
    "\n",
    "    Args:\n",
    "        arr (`List[int]`): non-empty sorted list of integers\n",
    "        x (`int`): query\n",
    "\n",
    "    Returns:\n",
    "        `int`: the position i so that arr[i] <= x < arr[i+1]\n",
    "\n",
    "    Raises:\n",
    "        `IndexError`: if the array is empty or if the query is outside the array values\n",
    "    \"\"\"\n",
    "    i, j = 0, len(arr) - 1\n",
    "    while i < j and arr[i] <= x < arr[j]:\n",
    "        k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\n",
    "        if arr[k] <= x < arr[k + 1]:\n",
    "            return k\n",
    "        elif arr[k] < x:\n",
    "            i, j = k + 1, j\n",
    "        else:\n",
    "            i, j = i, k\n",
    "    raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Non ci interessa fare lo sclicing di un vettore di batch che, di fatto, è già in memory. Dato il vettore di \n",
    "### offset (creato e storato come metadato in fase di scrittura partizionata) dobbiamo aprire on the fly la tabella (o il batch, che dir si voglia)\n",
    "### dato l'indice e l'informazione storata nell'offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stolen(index,batches):\n",
    "    offset=index\n",
    "    if offset < 0:\n",
    "        raise IndexError(\"Offset must be non-negative\")\n",
    "    elif offset >= offsets[-1] or (length is not None and length <= 0):\n",
    "        return pa.Table.from_batches([], schema=schema)\n",
    "    i = _interpolation_search(offsets, offset)\n",
    "    if length is None or length + offset >= offsets[-1]:\n",
    "        batches = batches[i:]\n",
    "        batches[0] = batches[0].slice(offset - offsets[i])\n",
    "    else:\n",
    "        j = _interpolation_search(offsets, offset + length - 1)\n",
    "        batches = batches[i : j + 1]\n",
    "        batches[-1] = batches[-1].slice(0, offset + length - offsets[j])\n",
    "        batches[0] = batches[0].slice(offset - offsets[i])\n",
    "    return pa.Table.from_batches(batches, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.partition_dataset(table,\"imagenette/imagenette2/output/partitioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "offsets = np.load(\"imagenette/imagenette2/output/partitioning/ds_offsets.npy\")\n",
    "index_file_to_open = _interpolation_search(offsets,800)\n",
    "#with pa.memory_map(f'{path}_{item}_stream.arrows', 'rb') as source:\n",
    "#                        table = pa.ipc.open_stream(source).read_all()\n",
    "#                        en_time_load = time.time()\n",
    "#                        tmp_load.append(en_time_load - st_time_load)\n",
    "print(len(offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(2,len(offsets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"imagenette/imagenette2/output/partitioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pa.memory_map(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "        table = pa.ipc.open_stream(source).read_all()\n",
    "\n",
    "for index_file_to_open in list(range(2,len(offsets))):\n",
    "        with pa.memory_map(f\"{root_path}/ds_{index_file_to_open}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                table = pa.concat_tables([table,pa.ipc.open_stream(source).read_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table.take([800-offsets[index_file_to_open]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "800-offsets[index_file_to_open]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_paths = [[\"image_feature\",\"class_feature\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(root_path : str,\n",
    "               sample_index : int,\n",
    "               all_data_in_memory : bool = False,\n",
    "               memory_map : bool = False,\n",
    "               mode : str = \"stream\"\n",
    "               ):\n",
    "    \n",
    "        offsets = np.load(f\"{root_path}/ds_offsets.npy\")\n",
    "\n",
    "        #### ALL DATA IN MEMORY ####\n",
    "\n",
    "        if all_data_in_memory and mode==\"stream\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.memory_map(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_stream(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "        \n",
    "        if all_data_in_memory and mode==\"stream\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.OSFile(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_stream(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "\n",
    "        if all_data_in_memory and mode==\"file\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.memory_map(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_file(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "\n",
    "        if all_data_in_memory and mode==\"file\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.OSFile(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_file(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "\n",
    "        #### PARTITIONING ####\n",
    "\n",
    "        index_file_to_open = interpolation_search(offsets,sample_index)\n",
    "\n",
    "        if not all_data_in_memory and mode==\"stream\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "        \n",
    "        if not all_data_in_memory and mode==\"stream\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "               \n",
    "        if not all_data_in_memory and mode==\"file\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "               \n",
    "\n",
    "        if not all_data_in_memory and mode==\"file\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "\n",
    "        if not all_data_in_memory:\n",
    "                sample_index = sample_index-offsets[index_file_to_open]\n",
    "\n",
    "        \n",
    "        return get_sample_features(table,sample_index,TEST_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_features(table : pa.Table,\n",
    "                        sample_index : int,\n",
    "                        feature_list_paths: List[List[str]],\n",
    "                        feature_list_indexes : List[List[List[int]]] = None,\n",
    "                ):\n",
    "\n",
    "    sample = {}\n",
    "    if feature_list_indexes:\n",
    "      feature_list_indexes = [item.insert(0,[sample_index]) for item in feature_list_indexes]\n",
    "      for path,index in zip(feature_list_paths,feature_list_indexes):\n",
    "        obj = None\n",
    "        for i, feature_name in enumerate(path):\n",
    "            if i == 0:\n",
    "                obj = table.column(feature_name).chunk(0).take(index[i]) \n",
    "            else:\n",
    "                obj = obj.values.field(feature_name).take(index[i])\n",
    "        sample[str(path[-1])] = obj.to_pylist()\n",
    "\n",
    "    elif not feature_list_indexes:\n",
    "      for path in feature_list_paths :\n",
    "        obj = None\n",
    "        for i, feature_name in enumerate(path):\n",
    "            if i == 0:\n",
    "                obj = table.column(feature_name).chunk(0).take([sample_index])   \n",
    "            else:\n",
    "                obj = obj.values.field(feature_name)\n",
    "        sample[str(path[-1])] = obj.to_pylist()\n",
    "    else:\n",
    "       raise TypeError(\"either list of indices or single index must be provided\")\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [0]}\n"
     ]
    }
   ],
   "source": [
    "print(get_sample(root_path,1,all_data_in_memory=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
