{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from src.benchmarkers import *\n",
    "from src.benchmarkersV2 import *\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from typing import *\n",
    "import framework_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = pq.read_table(r\"C:\\Users\\Cristiano Lavoro\\Downloads\\ds_10.parquet\")\n",
    "table = pq.read_table(r\"C:\\Users\\Cristiano Lavoro\\Desktop\\benchmarks\\imagenette\\imagenette2\\output\\ds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── image_feature\n",
      "    └── [0]\n",
      "        └── [0]\n",
      "            ├── class_feature\n",
      "            │   ├── [0]\n",
      "            │   │   └── label\n",
      "            ├── image\n",
      "            └── text_feature\n",
      "                └── [0]\n",
      "                    └── text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(draw_tree_schema(table.take([0]).to_pydict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = table.to_batches(300)\n",
    "save_path = \"imagenette/imagenette2/output/partitioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(batches):\n",
    "    with pa.OSFile(f\"{save_path}/ds_stream_{i+1}_of_{len(batches)}.arrows\", 'wb') as sink:  \n",
    "        with pa.ipc.new_stream(sink, batches[0].schema) as writer: \n",
    "            writer.write_batch(batch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = table.schema\n",
    "batches = [recordbatch for recordbatch in table.to_batches(300) if len(recordbatch) > 0]\n",
    "offsets = np.cumsum([0] + [len(b) for b in batches], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 320\n",
    "length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interpolation_search(arr: List[int], x: int) -> int:\n",
    "    \"\"\"\n",
    "    Return the position i of a sorted array so that arr[i] <= x < arr[i+1]\n",
    "\n",
    "    Args:\n",
    "        arr (`List[int]`): non-empty sorted list of integers\n",
    "        x (`int`): query\n",
    "\n",
    "    Returns:\n",
    "        `int`: the position i so that arr[i] <= x < arr[i+1]\n",
    "\n",
    "    Raises:\n",
    "        `IndexError`: if the array is empty or if the query is outside the array values\n",
    "    \"\"\"\n",
    "    i, j = 0, len(arr) - 1\n",
    "    while i < j and arr[i] <= x < arr[j]:\n",
    "        k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\n",
    "        if arr[k] <= x < arr[k + 1]:\n",
    "            return k\n",
    "        elif arr[k] < x:\n",
    "            i, j = k + 1, j\n",
    "        else:\n",
    "            i, j = i, k\n",
    "    raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Non ci interessa fare lo sclicing di un vettore di batch che, di fatto, è già in memory. Dato il vettore di \n",
    "### offset (creato e storato come metadato in fase di scrittura partizionata) dobbiamo aprire on the fly la tabella (o il batch, che dir si voglia)\n",
    "### dato l'indice e l'informazione storata nell'offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stolen(offset,batches):\n",
    "    if offset < 0:\n",
    "        raise IndexError(\"Offset must be non-negative\")\n",
    "    elif offset >= offsets[-1] or (length is not None and length <= 0):\n",
    "        return pa.Table.from_batches([], schema=schema)\n",
    "    i = _interpolation_search(offsets, offset)\n",
    "    if length is None or length + offset >= offsets[-1]:\n",
    "        batches = batches[i:]\n",
    "        batches[0] = batches[0].slice(offset - offsets[i])\n",
    "    else:\n",
    "        j = _interpolation_search(offsets, offset + length - 1)\n",
    "        batches = batches[i : j + 1]\n",
    "        batches[-1] = batches[-1].slice(0, offset + length - offsets[j])\n",
    "        batches[0] = batches[0].slice(offset - offsets[i])\n",
    "    return pa.Table.from_batches(batches, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stolen(offset,batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
