{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from src.benchmarkers import *\n",
    "from src.benchmarkersV2 import *\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from typing import *\n",
    "#import framework_functions as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = pq.read_table(r\"C:\\Users\\Cristiano Lavoro\\Downloads\\ds_10.parquet\")\n",
    "table = pq.read_table(r\"C:\\Users\\Cristiano Lavoro\\Desktop\\benchmarks\\imagenette\\imagenette2\\output\\ds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ff.draw_tree_schema(table.take([0]).to_pydict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_search(arr: List[int], x: int) -> int:\n",
    "    \"\"\"\n",
    "    Return the position i of a sorted array so that arr[i] <= x < arr[i+1]\n",
    "\n",
    "    Args:\n",
    "        arr (`List[int]`): non-empty sorted list of integers\n",
    "        x (`int`): query\n",
    "\n",
    "    Returns:\n",
    "        `int`: the position i so that arr[i] <= x < arr[i+1]\n",
    "\n",
    "    Raises:\n",
    "        `IndexError`: if the array is empty or if the query is outside the array values\n",
    "    \"\"\"\n",
    "    i, j = 0, len(arr) - 1\n",
    "    while i < j and arr[i] <= x < arr[j]:\n",
    "        k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\n",
    "        if arr[k] <= x < arr[k + 1]:\n",
    "            return k\n",
    "        elif arr[k] < x:\n",
    "            i, j = k + 1, j\n",
    "        else:\n",
    "            i, j = i, k\n",
    "    raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Non ci interessa fare lo sclicing di un vettore di batch che, di fatto, è già in memory. Dato il vettore di \n",
    "### offset (creato e storato come metadato in fase di scrittura partizionata) dobbiamo aprire on the fly la tabella (o il batch, che dir si voglia)\n",
    "### dato l'indice e l'informazione storata nell'offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(\n",
    "        table: pa.Table,\n",
    "        save_path : str,\n",
    "        max_batch_size : int = 300,\n",
    "        mode : str = \"stream\",\n",
    "        return_offsets : bool = False):\n",
    "    \n",
    "    batches = table.to_batches(max_batch_size)\n",
    "    offsets = np.cumsum([0] + [len(b) for b in batches], dtype=np.int64)\n",
    "    np.save(f\"{save_path}/ds_offsets.npy\",offsets)\n",
    "    if mode == \"stream\":\n",
    "        for i,batch in enumerate(batches):\n",
    "            with pa.OSFile(f\"{save_path}/ds_{i+1}_of_{len(batches)}_stream.arrows\", 'wb') as sink:  \n",
    "                with pa.ipc.new_stream(sink, batches[0].schema) as writer: \n",
    "                    writer.write_batch(batch)  \n",
    "    elif mode == \"file\":\n",
    "        for i,batch in enumerate(batches):\n",
    "            with pa.OSFile(f\"{save_path}/ds_{i+1}_of_{len(batches)}_file.arrow\", 'wb') as sink:  \n",
    "                with pa.ipc.new_file(sink, batches[0].schema) as writer: \n",
    "                    writer.write_batch(batch)\n",
    "    else:\n",
    "        raise NotImplementedError(\"mode not implemented\")\n",
    "    \n",
    "    if return_offsets:\n",
    "        return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"imagenette/imagenette2/output/partitioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_dataset(table,root_path,max_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_paths = [[\"image_feature\",\"boundingbox_feature\",\"bbox\"]]\n",
    "TEST_indices = [[[3],[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(root_path : str,\n",
    "               sample_index : int,\n",
    "               all_data_in_memory : bool = False,\n",
    "               memory_map : bool = False,\n",
    "               mode : str = \"stream\"\n",
    "               ):\n",
    "    \n",
    "        offsets = np.load(f\"{root_path}/ds_offsets.npy\")\n",
    "\n",
    "        #### ALL DATA IN MEMORY ####\n",
    "\n",
    "        if all_data_in_memory and mode==\"stream\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.memory_map(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_stream(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "        \n",
    "        if all_data_in_memory and mode==\"stream\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.OSFile(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_stream(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "\n",
    "        if all_data_in_memory and mode==\"file\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.memory_map(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_file(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "\n",
    "        if all_data_in_memory and mode==\"file\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "                for i in list(range(2,len(offsets))):\n",
    "                        with pa.OSFile(f\"{root_path}/ds_{i}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                                table = pa.concat_tables([table,pa.ipc.open_file(source).read_all()])\n",
    "                table = table.combine_chunks()\n",
    "\n",
    "        #### PARTITIONING ####\n",
    "\n",
    "        index_file_to_open = interpolation_search(offsets,sample_index)\n",
    "\n",
    "        if not all_data_in_memory and mode==\"stream\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "        \n",
    "        if not all_data_in_memory and mode==\"stream\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_stream.arrows\", 'rb') as source:\n",
    "                        table = pa.ipc.open_stream(source).read_all()\n",
    "               \n",
    "        if not all_data_in_memory and mode==\"file\" and memory_map:\n",
    "                with pa.memory_map(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "               \n",
    "\n",
    "        if not all_data_in_memory and mode==\"file\" and not memory_map:\n",
    "                with pa.OSFile(f\"{root_path}/ds_{index_file_to_open+1}_of_{len(offsets)-1}_file.arrow\", 'rb') as source:\n",
    "                        table = pa.ipc.open_file(source).read_all()\n",
    "\n",
    "        if not all_data_in_memory:\n",
    "                sample_index = sample_index-offsets[index_file_to_open]\n",
    "\n",
    "        TEST_indices = [[[1],[0]]]\n",
    "        return get_sample_features(table,sample_index,TEST_paths,feature_list_indexes=TEST_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_features(table : pa.Table,\n",
    "                        sample_index : int,\n",
    "                        feature_list_paths: List[List[str]],\n",
    "                        feature_list_indexes : List[List[List[int]]] = None,\n",
    "                ):\n",
    "\n",
    "    sample = {}\n",
    "    if feature_list_indexes:\n",
    "      for item in feature_list_indexes:\n",
    "        item.insert(0,[sample_index])\n",
    "      for path,index in zip(feature_list_paths,feature_list_indexes):\n",
    "        obj = None\n",
    "        for i, feature_name in enumerate(path):\n",
    "            if i == 0:\n",
    "                obj = table.column(feature_name).chunk(0).take(index[i]) \n",
    "            else:\n",
    "                obj = obj.values.field(feature_name).take(index[i])\n",
    "        sample[str(path[-1])] = obj.to_pylist()\n",
    "\n",
    "    elif not feature_list_indexes:\n",
    "      for path in feature_list_paths :\n",
    "        obj = None\n",
    "        for i, feature_name in enumerate(path):\n",
    "            if i == 0:\n",
    "                obj = table.column(feature_name).chunk(0).take([sample_index])   \n",
    "            else:\n",
    "                obj = obj.values.field(feature_name)\n",
    "        sample[str(path[-1])] = obj.to_pylist()\n",
    "    else:\n",
    "       raise TypeError(\"either list of indices or single index must be provided\")\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bbox': [[0.0977, 0.2604, 0.8789, 0.7812, 1.0]]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(root_path,0,all_data_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in TEST_indices:\n",
    "    item.insert(0,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0], [1], [1]]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tree_schema(data, indent=\"\", is_last=True):\n",
    "    output = \"\"\n",
    "    if isinstance(data, dict):\n",
    "        items = list(data.items())\n",
    "        for i, (key, value) in enumerate(items):\n",
    "            connector = \"└── \" if is_last and i == len(items) - 1 else \"├── \"\n",
    "            output += f\"{indent}{connector}{key}\\n\"\n",
    "            new_indent = indent + (\"    \" if is_last and i == len(items) - 1 else \"│   \")\n",
    "            output += draw_tree_schema(value, new_indent, i == len(items) - 1)\n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data):\n",
    "            connector = \"└── \" if is_last and i == len(data) - 1 else \"├── \"\n",
    "            output += f\"{indent}{connector}[{i}]\\n\"\n",
    "            new_indent = indent + (\"    \" if is_last and i == len(data) - 1 else \"│   \")\n",
    "            output += draw_tree_schema(item, new_indent, i == len(data) - 1)\n",
    "    return output\n",
    "\n",
    "def make_offset(vector):\n",
    "    return [vector[i] - vector[i - 1] for i in range(1, len(vector))]\n",
    "\n",
    "def group_objects(objects, cardinality_list):\n",
    "    result = []\n",
    "    index = 0\n",
    "    for count in cardinality_list:\n",
    "        if count == 0:\n",
    "            result.append(None)\n",
    "        if count > 0:  # Only create a group if count > 0\n",
    "            result.append(objects[index:index+count])\n",
    "            index += count\n",
    "        if index > len(objects):\n",
    "            result.append(None)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(table : pa.Table,\n",
    "              feature_list_path: List[str],\n",
    "              feature_to_add_name : str,\n",
    "              feature_to_add_values : List[List[Dict]]\n",
    "              ):\n",
    "    \n",
    "    obj = None\n",
    "    to_revert_paths = []\n",
    "\n",
    "    for indx, feature_name in enumerate(feature_list_path):\n",
    "        if indx == 0:\n",
    "            obj = table.column(feature_name).chunk(0)\n",
    "        else:\n",
    "            obj = obj.values.field(feature_name)\n",
    "        \n",
    "        to_revert_paths.append(obj)\n",
    "\n",
    "        if indx == len(feature_list_path) - 1:\n",
    "            obj = obj.values.field(feature_to_add_name)\n",
    "    \n",
    "    new_struct = None\n",
    "    field_tmp = obj.values\n",
    "    cardinality_list = make_offset(obj.offsets.to_pylist())\n",
    "    field_tmp = group_objects(field_tmp.to_pylist(), cardinality_list)\n",
    "    #new_field = pa.array([item + feature_to_add_values for item in field_tmp])\n",
    "    #return [item + feature_to_add_values for item in field_tmp]\n",
    "    new_field = []\n",
    "    for i,j in zip(field_tmp,feature_to_add_values):\n",
    "        new_field.append(i+j)\n",
    "    new_field = pa.array(new_field)\n",
    "    for indx,level in enumerate(list(reversed(to_revert_paths))):\n",
    "        if indx!=0:\n",
    "            new_field = new_struct\n",
    "            feature_to_add_name = list(reversed(feature_list_path))[indx - 1]\n",
    "\n",
    "        level_fields_values = [new_field]\n",
    "        level_fields_names = [feature_to_add_name]\n",
    "\n",
    "        for x in list(level.values.type):\n",
    "            if not x.name == feature_to_add_name:\n",
    "                level_fields_names.append(x.name)\n",
    "                level_fields_values.append(level.values.field(x.name))    \n",
    "        new_struct_no_cardinality = pa.StructArray.from_arrays(\n",
    "            level_fields_values,  \n",
    "            names=level_fields_names\n",
    "        )\n",
    "        cardinality_list = make_offset(level.offsets.to_pylist())\n",
    "        new_struct = pa.array(group_objects(new_struct_no_cardinality.to_pylist(), cardinality_list))\n",
    "        \n",
    "        if indx == len(list(reversed(to_revert_paths))) - 1:\n",
    "            return table.set_column(table.schema.get_field_index(list(reversed(feature_list_path))[indx]), list(reversed(feature_list_path))[indx], new_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_merge = [[{\"label\":i}] for i in range(3925)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = add_feature(table,[\"image_feature\"],\"class_feature\",feature_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ListArray object at 0x000001FA1397FDC0>\n",
       "[\n",
       "  -- is_valid: all not null\n",
       "  -- child 0 type: int64\n",
       "    [\n",
       "      0,\n",
       "      0\n",
       "    ],\n",
       "  -- is_valid: all not null\n",
       "  -- child 0 type: int64\n",
       "    [\n",
       "      0,\n",
       "      1\n",
       "    ],\n",
       "  ...\n",
       "  -- is_valid: all not null\n",
       "  -- child 0 type: int64\n",
       "    [\n",
       "      9,\n",
       "      3923\n",
       "    ],\n",
       "  -- is_valid: all not null\n",
       "  -- child 0 type: int64\n",
       "    [\n",
       "      9,\n",
       "      3924\n",
       "    ]\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova.column(\"image_feature\").chunk(0).values.field(\"class_feature\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(table : pa.Table,\n",
    "              feature_list_path: List[str],\n",
    "              new_feature_name : str,\n",
    "              new_feature : List[List[Dict]],\n",
    "              ):\n",
    "    \"\"\"\n",
    "    This function permits to add a feature at the level specified by the last string contained in the attribute \"feature_list_path\"\n",
    "    \"\"\"\n",
    "    obj = None\n",
    "    to_revert_paths = []\n",
    "\n",
    "    for indx, feature_name in enumerate(feature_list_path):\n",
    "        if indx == 0:\n",
    "            obj = table.column(feature_name).chunk(0)\n",
    "        else:\n",
    "            obj = obj.values.field(feature_name)\n",
    "        \n",
    "        to_revert_paths.append(obj)\n",
    "\n",
    "        #if indx == len(feature_list_path) - 1:\n",
    "        #    obj = obj.values.field(feature_to_manipulate)\n",
    "\n",
    "    new_field = pa.array(new_feature)\n",
    "    print(len(new_field))\n",
    "    new_struct = None\n",
    "\n",
    "    for indx,level in enumerate(list(reversed(to_revert_paths))):\n",
    "        if indx!=0:\n",
    "            new_field = new_struct\n",
    "            new_feature_name = list(reversed(feature_list_path))[indx - 1]\n",
    "        level_fields_values = [new_field]\n",
    "        level_fields_names = [new_feature_name]\n",
    "        \n",
    "        for x in list(level.values.type):\n",
    "            if not x.name == new_feature_name:\n",
    "                level_fields_names.append(x.name)\n",
    "                level_fields_values.append(level.values.field(x.name))    \n",
    "        new_struct_no_cardinality = pa.StructArray.from_arrays(\n",
    "            level_fields_values,  \n",
    "            names=level_fields_names\n",
    "        )\n",
    "        cardinality_list = make_offset(level.offsets.to_pylist())\n",
    "        new_struct = pa.array(group_objects(new_struct_no_cardinality.to_pylist(), cardinality_list))\n",
    "        \n",
    "        if indx == len(list(reversed(to_revert_paths))) - 1:\n",
    "            return table.set_column(table.schema.get_field_index(list(reversed(feature_list_path))[indx]), list(reversed(feature_list_path))[indx], new_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3925\n"
     ]
    }
   ],
   "source": [
    "prova = add_feature(table,[\"image_feature\",\"class_feature\"],\"boundingbox_feature\",[[{\"boundingbox_feature\":[{\"bbox\":[0.5,0.5,0.5,0.5,0]}]}] for i in range(3925)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pyarrow.Field<boundingbox_feature: list<item: struct<bbox: list<item: double>>>>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(prova.column(\"image_feature\").chunk(0).values.field(\"class_feature\").values.field(\"boundingbox_feature\").values.type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
